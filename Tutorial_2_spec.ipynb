{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "BIYexixg588i",
        "XOqA-RtjUn-_"
      ],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s24imlon/comp70028_rl/blob/main/Tutorial_2_spec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "20IyxDzgp3tU"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt # Graphical library"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyuTicLD6F2z"
      },
      "source": [
        "# Lab Assignment 2 :  \n",
        "See pdf for instructions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIYexixg588i"
      },
      "source": [
        "## Helper Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWnMW3GNpjd7"
      },
      "source": [
        "# This class is used ONLY for graphics\n",
        "# YOU DO NOT NEED to understand it to work on this lab assessment\n",
        "\n",
        "class GraphicsGridWorld(object):\n",
        "\n",
        "  def __init__(self, shape, locations, obstacle_locs, absorbing_locs, absorbing_rewards, absorbing):\n",
        "\n",
        "    # Ensure all inputs are valids using the is_valid_grid function\n",
        "    is_valid_grid(shape, 0, obstacle_locs, absorbing_locs, absorbing_rewards)\n",
        "\n",
        "    self.shape = shape\n",
        "    self.locations = locations\n",
        "    self.absorbing = absorbing\n",
        "\n",
        "    # Walls\n",
        "    self.walls = np.zeros(self.shape)\n",
        "    for ob in obstacle_locs:\n",
        "      self.walls[ob] = 1\n",
        "\n",
        "    # Absorbing states\n",
        "    self.absorbers = np.zeros(self.shape)\n",
        "    for ab in absorbing_locs:\n",
        "      self.absorbers[ab] = -1\n",
        "\n",
        "    # Rewards\n",
        "    self.rewarders = np.zeros(self.shape)\n",
        "    for i, rew in enumerate(absorbing_locs):\n",
        "      self.rewarders[rew] = absorbing_rewards[i]\n",
        "\n",
        "    # Print the map to show it\n",
        "    self.paint_maps()\n",
        "\n",
        "  def paint_maps(self):\n",
        "    \"\"\"\n",
        "    Print the Grid topology (obstacles, absorbing states and rewards)\n",
        "    input: /\n",
        "    output: /\n",
        "    \"\"\"\n",
        "    plt.figure()\n",
        "    plt.subplot(1,3,1)\n",
        "    plt.imshow(self.walls)\n",
        "    plt.title('Obstacles')\n",
        "    plt.subplot(1,3,2)\n",
        "    plt.imshow(self.absorbers)\n",
        "    plt.title('Absorbing states')\n",
        "    plt.subplot(1,3,3)\n",
        "    plt.imshow(self.rewarders)\n",
        "    plt.title('Reward states')\n",
        "    plt.show()\n",
        "\n",
        "  def draw_deterministic_policy(self, Policy):\n",
        "    \"\"\"\n",
        "    Draw a deterministic policy\n",
        "    input: Policy {np.array} -- policy to draw (should be an array of values between 0 and 3 (actions))\n",
        "    output: /\n",
        "    \"\"\"\n",
        "    plt.figure()\n",
        "    plt.imshow(self.walls + self.rewarders + self.absorbers) # Create the graph of the grid\n",
        "    for state, action in enumerate(Policy):\n",
        "      if(self.absorbing[0,state]): # If it is an absorbing state, don't plot any action\n",
        "        continue\n",
        "      arrows = [r\"$\\uparrow$\",r\"$\\rightarrow$\", r\"$\\downarrow$\", r\"$\\leftarrow$\"] # List of arrows corresponding to each possible action\n",
        "      action_arrow = arrows[action] # Take the corresponding action\n",
        "      location = self.locations[state] # Compute its location on graph\n",
        "      plt.text(location[1], location[0], action_arrow, ha='center', va='center') # Place it on graph\n",
        "    plt.show()\n",
        "\n",
        "  def draw_policy(self, Policy):\n",
        "    \"\"\"\n",
        "    Draw a policy (draw an arrow in the most probable direction)\n",
        "    input: Policy {np.array} -- policy to draw as probability\n",
        "    output: /\n",
        "    \"\"\"\n",
        "    deterministic_policy = np.array([np.argmax(Policy[row,:]) for row in range(Policy.shape[0])])\n",
        "    self.draw_deterministic_policy(deterministic_policy)\n",
        "\n",
        "  def draw_value(self, Value):\n",
        "    \"\"\"\n",
        "    Draw a policy value\n",
        "    input: Value {np.array} -- policy values to draw\n",
        "    output: /\n",
        "    \"\"\"\n",
        "    plt.figure()\n",
        "    plt.imshow(self.walls + self.rewarders + self.absorbers) # Create the graph of the grid\n",
        "    for state, value in enumerate(Value):\n",
        "      if(self.absorbing[0, state]): # If it is an absorbing state, don't plot any value\n",
        "        continue\n",
        "      location = self.locations[state] # Compute the value location on graph\n",
        "      plt.text(location[1], location[0], round(value,2), ha='center', va='center') # Place it on graph\n",
        "    plt.show()\n",
        "\n",
        "  def draw_deterministic_policy_grid(self, Policies, title, n_columns, n_lines):\n",
        "    \"\"\"\n",
        "    Draw a grid representing multiple deterministic policies\n",
        "    input: Policies {np.array of np.array} -- array of policies to draw (each should be an array of values between 0 and 3 (actions))\n",
        "    output: /\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(20,8))\n",
        "    for subplot in range (len(Policies)): # Go through all policies\n",
        "      ax = plt.subplot(n_columns, n_lines, subplot+1) # Create a subplot for each policy\n",
        "      ax.imshow(self.walls+self.rewarders +self.absorbers) # Create the graph of the grid\n",
        "      for state, action in enumerate(Policies[subplot]):\n",
        "        if(self.absorbing[0,state]): # If it is an absorbing state, don't plot any action\n",
        "          continue\n",
        "        arrows = [r\"$\\uparrow$\",r\"$\\rightarrow$\", r\"$\\downarrow$\", r\"$\\leftarrow$\"] # List of arrows corresponding to each possible action\n",
        "        action_arrow = arrows[action] # Take the corresponding action\n",
        "        location = self.locations[state] # Compute its location on graph\n",
        "        plt.text(location[1], location[0], action_arrow, ha='center', va='center') # Place it on graph\n",
        "      ax.title.set_text(title[subplot]) # Set the title for the graph given as argument\n",
        "    plt.show()\n",
        "\n",
        "  def draw_policy_grid(self, Policies, title, n_columns, n_lines):\n",
        "    \"\"\"\n",
        "    Draw a grid representing multiple policies (draw an arrow in the most probable direction)\n",
        "    input: Policy {np.array} -- array of policies to draw as probability\n",
        "    output: /\n",
        "    \"\"\"\n",
        "    deterministic_policies = np.array([[np.argmax(Policy[row,:]) for row in range(Policy.shape[0])] for Policy in Policies])\n",
        "    self.draw_deterministic_policy_grid(deterministic_policies, title, n_columns, n_lines)\n",
        "\n",
        "  def draw_value_grid(self, Values, title, n_columns, n_lines):\n",
        "    \"\"\"\n",
        "    Draw a grid representing multiple policy values\n",
        "    input: Values {np.array of np.array} -- array of policy values to draw\n",
        "    output: /\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(20,8))\n",
        "    for subplot in range (len(Values)): # Go through all values\n",
        "      ax = plt.subplot(n_columns, n_lines, subplot+1) # Create a subplot for each value\n",
        "      ax.imshow(self.walls+self.rewarders +self.absorbers) # Create the graph of the grid\n",
        "      for state, value in enumerate(Values[subplot]):\n",
        "        if(self.absorbing[0,state]): # If it is an absorbing state, don't plot any value\n",
        "          continue\n",
        "        location = self.locations[state] # Compute the value location on graph\n",
        "        plt.text(location[1], location[0], round(value,1), ha='center', va='center') # Place it on graph\n",
        "      ax.title.set_text(title[subplot]) # Set the title for the graoh given as argument\n",
        "    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMKhnaBrstG-"
      },
      "source": [
        "# Here, we define some functions used across the code\n",
        "# YOU DO NOT NEED to understand it to work on this lab assessment\n",
        "\n",
        "def is_valid_grid(shape, prob_success, obstacle_locs, absorbing_locs, absorbing_rewards):\n",
        "  \"\"\"\n",
        "  Ensure the property defining the grid are all valid\n",
        "  inputs: all properties to initialise a GridWorld (see GridWorld class)\n",
        "  outputs: /\n",
        "  \"\"\"\n",
        "  assert len(shape) == 2, \"The grid should be two dimensions.\"\n",
        "  assert (prob_success <= 1) and (prob_success >= 0), \"Probability of action success should be in [0, 1].\"\n",
        "  for obstacle_loc in obstacle_locs:\n",
        "    assert len(obstacle_loc) == 2, \"The obstacle locations should have two coordinates.\"\n",
        "    assert (obstacle_loc[0] < shape[0]) and (obstacle_loc[1] < shape[1]), \"The obstacle locations should be inside the grid.\"\n",
        "  assert len(absorbing_locs) == len(absorbing_rewards), \"The absorbing_locs and absorbing_rewards should have the same length.\"\n",
        "  for absorbing_loc in absorbing_locs:\n",
        "    assert len(absorbing_loc) == 2, \"The absorbing locations should have two coordinates.\"\n",
        "    assert (absorbing_loc[0] < shape[0]) and (absorbing_loc[1] < shape[1]), \"The absorbing locations should be inside the grid.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOqA-RtjUn-_"
      },
      "source": [
        "## GridWorld"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXc1OFvZqJfZ"
      },
      "source": [
        "# This class define the Grid world\n",
        "\n",
        "class GridWorld(object):\n",
        "\n",
        "  def __init__(self,\n",
        "               shape = (5,5),\n",
        "               prob_success = 0.7,\n",
        "               obstacle_locs = [(1,1),(2,1),(2,3)],\n",
        "               absorbing_locs = [(4,0),(4,1),(4,2),(4,3),(4,4)],\n",
        "               absorbing_rewards = [-10, -10, -10, -10, 10]\n",
        "              ):\n",
        "    \"\"\"\n",
        "    GridWorld initialisation\n",
        "    input:\n",
        "      - shape {tuple} -- GridWorld shape (height, width)\n",
        "      - prob_success {float} -- probability of success when taking an action, used to fill the transition matrix\n",
        "      - obstacle_locs {list of tuples} -- location of all obstacles of the grid: [(obstacle 1), (obstacle 2), ...]\n",
        "      - absorbing_locs {list of tuples} -- location of all absorbing states of the grid: [(state 1), (state 2), ...]\n",
        "      - absorbing_rewards {list of float} -- reward corresponding to eacg absorbing state of the grid: [reward 1, reward 2, ...]\n",
        "    output: /\n",
        "    \"\"\"\n",
        "\n",
        "    # Ensure all inputs are valids using the is_valid_grid function\n",
        "    is_valid_grid(shape, prob_success, obstacle_locs, absorbing_locs, absorbing_rewards)\n",
        "\n",
        "    # Setting inputs as attributes\n",
        "    self.shape = shape\n",
        "    self.prob_success = prob_success\n",
        "    self.obstacle_locs = obstacle_locs\n",
        "    self.absorbing_locs = absorbing_locs\n",
        "    self.absorbing_rewards = absorbing_rewards\n",
        "\n",
        "    # Actions\n",
        "    self.action_size = 4\n",
        "    self.direction_names = ['N','E','S','W'] # Direction 0 is 'N', 1 is 'E' and so on\n",
        "\n",
        "    # States\n",
        "    self.locations = self.fill_in_states()\n",
        "    self.state_size = len(self.locations)\n",
        "    self.neighbours = self.fill_in_neighbours()\n",
        "    self.absorbing = self.fill_in_absorbing()\n",
        "\n",
        "    # Transition and reward matrices\n",
        "    self.T = self.fill_in_transition()\n",
        "    self.R = self.fill_in_reward()\n",
        "\n",
        "    # Creating the graphical grid world\n",
        "    self.graphics = GraphicsGridWorld(self.shape, self.locations, self.obstacle_locs, self.absorbing_locs, self.absorbing_rewards, self.absorbing)\n",
        "\n",
        "\n",
        "  def is_location(self, loc):\n",
        "    \"\"\"\n",
        "    Is the location a valid state (not out of grid and not an obstacle)\n",
        "    input: loc {tuple} -- location of the state\n",
        "    output: _ {bool} -- is the location a valid state\n",
        "    \"\"\"\n",
        "    if (loc[0] < 0 or loc[1] < 0 or loc[0] > self.shape[0]-1 or loc[1] > self.shape[1]-1):\n",
        "      return False\n",
        "    elif (loc in self.obstacle_locs):\n",
        "      return False\n",
        "    else:\n",
        "      return True\n",
        "\n",
        "\n",
        "  def get_state_from_loc(self, loc):\n",
        "    \"\"\"\n",
        "    Get the state number corresponding to a given location\n",
        "    input: loc {tuple} -- location of the state\n",
        "    output: index {int} -- corresponding state number\n",
        "    \"\"\"\n",
        "    return self.locations.index(tuple(loc))\n",
        "\n",
        "\n",
        "  def get_loc_from_state(self, state):\n",
        "    \"\"\"\n",
        "    Get the state number corresponding to a given location\n",
        "    input: index {int} -- state number\n",
        "    output: loc {tuple} -- corresponding location\n",
        "    \"\"\"\n",
        "    return self.locations[state]\n",
        "\n",
        "\n",
        "  def fill_in_states(self):\n",
        "    \"\"\"\n",
        "    Build the states numbers\n",
        "    input: /\n",
        "    output: locations {list of tuples} -- mapping from location to state number\n",
        "    \"\"\"\n",
        "\n",
        "    locations = []\n",
        "    for i in range (self.shape[0]):\n",
        "      for j in range (self.shape[1]):\n",
        "        loc = (i,j)\n",
        "        # Adding the state to locations if it is no obstacle\n",
        "        if self.is_location(loc):\n",
        "          locations.append(loc)\n",
        "    return locations\n",
        "\n",
        "\n",
        "  def fill_in_neighbours(self):\n",
        "    \"\"\"\n",
        "    Build the neighbouring states in the grid\n",
        "    input: /\n",
        "    output: neighbours {np.array} -- matrix containing the state number of the neighbours in each direction of all states\n",
        "    \"\"\"\n",
        "\n",
        "    # Each line is a state, ranked by state-number, each column is a direction (N, E, S, W)\n",
        "    neighbours = np.zeros((self.state_size, 4))\n",
        "\n",
        "    for state in range(self.state_size):\n",
        "      loc = self.get_loc_from_state(state)\n",
        "\n",
        "      # North\n",
        "      neighbour = (loc[0]-1, loc[1]) # North neighbours location\n",
        "      if self.is_location(neighbour):\n",
        "        neighbours[state][self.direction_names.index('N')] = self.get_state_from_loc(neighbour)\n",
        "      else: # If there is no neighbour in this direction, coming back to current state\n",
        "        neighbours[state][self.direction_names.index('N')] = state\n",
        "\n",
        "      # East\n",
        "      neighbour = (loc[0], loc[1]+1) # East neighbours location\n",
        "      if self.is_location(neighbour):\n",
        "        neighbours[state][self.direction_names.index('E')] = self.get_state_from_loc(neighbour)\n",
        "      else: # If there is no neighbour in this direction, coming back to current state\n",
        "        neighbours[state][self.direction_names.index('E')] = state\n",
        "\n",
        "      # South\n",
        "      neighbour = (loc[0]+1, loc[1]) # South neighbours location\n",
        "      if self.is_location(neighbour):\n",
        "        neighbours[state][self.direction_names.index('S')] = self.get_state_from_loc(neighbour)\n",
        "      else: # If there is no neighbour in this direction, coming back to current state\n",
        "        neighbours[state][self.direction_names.index('S')] = state\n",
        "\n",
        "      # West\n",
        "      neighbour = (loc[0], loc[1]-1) # West neighbours location\n",
        "      if self.is_location(neighbour):\n",
        "        neighbours[state][self.direction_names.index('W')] = self.get_state_from_loc(neighbour)\n",
        "      else: # If there is no neighbour in this direction, coming back to current state\n",
        "        neighbours[state][self.direction_names.index('W')] = state\n",
        "\n",
        "    return neighbours\n",
        "\n",
        "\n",
        "  def fill_in_absorbing(self):\n",
        "    \"\"\"\n",
        "    Translate absorbing locations into absorbing state indices\n",
        "    input: /\n",
        "    output: absorbing {np.array} -- array with value 1 when the state is absorbing the state number of the neighbours in each direction of all states\n",
        "    \"\"\"\n",
        "    absorbing = np.zeros((1, self.state_size))\n",
        "    for a in self.absorbing_locs:\n",
        "      absorbing_state = self.get_state_from_loc(a)\n",
        "      absorbing[0, absorbing_state] = 1\n",
        "    return absorbing\n",
        "\n",
        "\n",
        "  # [Action required]\n",
        "  def fill_in_transition(self):\n",
        "    \"\"\"\n",
        "    Compute the transition matrix of the grid\n",
        "    input: /\n",
        "    output: T {np.array} -- the transition matrix of the grid\n",
        "    \"\"\"\n",
        "    T = np.zeros((self.state_size, self.state_size, self.action_size)) # Empty matrix of dimension S*S*A\n",
        "\n",
        "    ####\n",
        "    # Add your code here\n",
        "    # Hint! You might need: self.action_size, self.state_size, self.neighbours, self.prob_success\n",
        "    ####\n",
        "\n",
        "    return T\n",
        "\n",
        "  # [Action required]\n",
        "  def fill_in_reward(self):\n",
        "    \"\"\"\n",
        "    Compute the reward matrix of the grid\n",
        "    input: /\n",
        "    output: R {np.array} -- the reward matrix of the grid\n",
        "    \"\"\"\n",
        "    R = np.ones((self.state_size, self.state_size, self.action_size)) # Matrix filled with 1\n",
        "\n",
        "    ####\n",
        "    # Add your code here\n",
        "    # Hint! You might need: self.absorbing_rewards, self.absorbing_locs, self.get_state_from_loc()\n",
        "    ####\n",
        "\n",
        "    return R\n",
        "\n",
        "\n",
        "\n",
        "  # [Action required]\n",
        "  def policy_evaluation(self, policy, threshold = 0.0001, gamma = 0.8):\n",
        "    \"\"\"\n",
        "    Policy evaluation on GridWorld\n",
        "    input:\n",
        "      - policy {np.array} -- policy to evaluate\n",
        "      - threshold {float} -- threshold value used to stop the policy evaluation algorithm\n",
        "      - gamma {float} -- discount factor\n",
        "    output:\n",
        "      - V {np.array} -- value function corresponding to the policy\n",
        "      - epochs {int} -- number of epochs to find this value function\n",
        "    \"\"\"\n",
        "\n",
        "    # Ensure inputs are valid\n",
        "    assert (policy.shape[0] == self.state_size) and (policy.shape[1] == self.action_size), \"The dimensions of the policy are not valid.\"\n",
        "    assert (gamma <=1) and (gamma >= 0), \"Discount factor should be in [0, 1].\"\n",
        "\n",
        "    # Initialisation\n",
        "    delta = 2*threshold # Ensure delta is bigger than the threshold to start the loop\n",
        "    V = np.zeros(self.state_size) # Initialise value function to 0\n",
        "    epoch = 0\n",
        "\n",
        "    ####\n",
        "    # Add your code here\n",
        "    # Hint! You might need: self.state_size, self.action_size, self.T, self.R, self.absorbing\n",
        "    ####\n",
        "\n",
        "    return V, epoch\n",
        "\n",
        "  # [Action required]\n",
        "  def policy_iteration(self, threshold = 0.0001, gamma = 0.8):\n",
        "    \"\"\"\n",
        "    Policy iteration on GridWorld\n",
        "    input:\n",
        "      - threshold {float} -- threshold value used to stop the policy iteration algorithm\n",
        "      - gamma {float} -- discount factor\n",
        "    output:\n",
        "      - policy {np.array} -- policy found using the policy iteration algorithm\n",
        "      - V {np.array} -- value function corresponding to the policy\n",
        "      - epochs {int} -- number of epochs to find this policy\n",
        "    \"\"\"\n",
        "\n",
        "    # Ensure gamma value is valid\n",
        "    assert (gamma <=1) and (gamma >= 0), \"Discount factor should be in [0, 1].\"\n",
        "\n",
        "    # Initialisation\n",
        "    policy = np.zeros((self.state_size, self.action_size)) # Vector of 0\n",
        "    policy[:, 0] = 1 # Initialise policy to choose action 1 systematically\n",
        "    V = np.zeros(self.state_size) # Initialise value function to 0\n",
        "    epochs = 0\n",
        "    policy_stable = False # Condition to stop the main loop\n",
        "\n",
        "    ####\n",
        "    # Add your code here\n",
        "    # Hint! You might need: self.state_size, self.action_size, self.T, self.R, self.absorbing, self.policy_evaluation()\n",
        "    ####\n",
        "\n",
        "    return policy, V, epochs\n",
        "\n",
        "  # [Action required]\n",
        "  def value_iteration(self, threshold = 0.0001, gamma = 0.8):\n",
        "    \"\"\"\n",
        "    Value iteration on GridWorld\n",
        "    input:\n",
        "      - threshold {float} -- threshold value used to stop the value iteration algorithm\n",
        "      - gamma {float} -- discount factor\n",
        "    output:\n",
        "      - policy {np.array} -- optimal policy found using the value iteration algorithm\n",
        "      - V {np.array} -- value function corresponding to the policy\n",
        "      - epochs {int} -- number of epochs to find this policy\n",
        "    \"\"\"\n",
        "\n",
        "    # Ensure gamma value is valid\n",
        "    assert (gamma <=1) and (gamma >= 0), \"Discount factor should be in [0, 1].\"\n",
        "\n",
        "    # Initialisation\n",
        "    epochs = 0\n",
        "    delta = threshold # Setting value of delta to go through the first breaking condition\n",
        "    V = np.zeros(self.state_size) # Initialise values at 0 for each state\n",
        "    policy = np.zeros((self.state_size, self.action_size)) # Initialisation\n",
        "\n",
        "    ####\n",
        "    # Add your code here\n",
        "    # Hint! You might need: self.state_size, self.action_size, self.T, self.R, self.absorbing\n",
        "    ####\n",
        "\n",
        "    return policy, V, epochs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJZHGUz55V84"
      },
      "source": [
        "## Question 1: Grid World Definition\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "scrolled": false,
        "id": "eyeJfvwXp3ta"
      },
      "source": [
        "### Question 1: Grid World definition\n",
        "\n",
        "# Define the grid\n",
        "print(\"Creating the Grid world:\\n\")\n",
        "grid = GridWorld()\n",
        "\n",
        "# Exemple prints to help you understand the structure of the transition matrix\n",
        "print(\"\\nThe first dimension of the transition matrix is:\", len(grid.T))\n",
        "print(\"The second dimension of the transition matrix is:\", len(grid.T[0]))\n",
        "print(\"The third dimension of the transition matrix is:\", len(grid.T[0,0]))\n",
        "print(\"The probability to go from state 0 to state 1 with action 0 (N) is:\", grid.T[0,1,0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0saHSxku5gMQ"
      },
      "source": [
        "## Question 2: policy evaluation implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "f3238vrZp3ti"
      },
      "source": [
        "### Question 2: policy evaluation implementation\n",
        "\n",
        "# Define the policy\n",
        "Policy = np.zeros((grid.state_size, grid.action_size))\n",
        "Policy = Policy + 0.25\n",
        "\n",
        "# Do not plot a graphical representation for this policy as it is fully random\n",
        "#print(\"Considering the uniform (unbiased) policy:\\n\\n {}\".format(Policy))\n",
        "\n",
        "# Policy evaluation with threshold = 0.001 and gamma = 0.8\n",
        "V, epochs = grid.policy_evaluation(Policy, threshold = 0.001, gamma = 0.8)\n",
        "\n",
        "# Plot value function\n",
        "print(\"The value of the uniform policy with gamma = 0.8 is:\\n\\n {}\".format(V))\n",
        "print(\"\\n\\nIts graphical representation is:\\n\")\n",
        "grid.graphics.draw_value(V)\n",
        "\n",
        "# Plot number of epochs\n",
        "print(\"\\nIt took {} epochs\".format(epochs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bk1oDM825oPE"
      },
      "source": [
        "## Question 3: impact of gamma on the policy evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "VF4ilbcrp3tg"
      },
      "source": [
        "### Question 3: impact of gamma on the policy evaluation\n",
        "\n",
        "gamma_range = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
        "epochs = []\n",
        "values = []\n",
        "titles = []\n",
        "\n",
        "# Use policy evaluation for each gamma value\n",
        "for gamma in gamma_range:\n",
        "    V, epoch = grid.policy_evaluation(Policy, threshold = 0.001, gamma = gamma)\n",
        "    epochs.append(epoch)\n",
        "    values.append(V)\n",
        "    titles.append(\"gamma = {}\".format(gamma))\n",
        "\n",
        "# Plot the number of epochs vs gamma values\n",
        "print(\"Impact of gamma value on the number of epochs needed for the policy evaluation algorithm:\\n\")\n",
        "plt.figure()\n",
        "plt.plot(gamma_range, epochs)\n",
        "plt.xlabel(\"Gamma\")\n",
        "plt.ylabel(\"Number of epochs\")\n",
        "plt.show()\n",
        "\n",
        "# Print all value functions for different values of gamma\n",
        "print(\"\\nGraphical representation of the value function for each gamma:\\n\")\n",
        "grid.graphics.draw_value_grid(values, titles, 2, 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFuN72Li5sVA"
      },
      "source": [
        "## Question 4: policy iteration implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "gsK5TzK3p3tl"
      },
      "source": [
        "### Question 4: policy iteration implementation\n",
        "\n",
        "# Policy iteration algorithm\n",
        "policy, V, epochs = grid.policy_iteration(threshold = 0.001, gamma = 0.7)\n",
        "\n",
        "# Plot value function for policy iteration\n",
        "#print(\"The value of the optimal value computed using policy iteration is:\\n\\n {}\\n\\n\".format(V))\n",
        "print(\"The graphical representation of the optimal value computed using policy iteration is:\\n\")\n",
        "grid.graphics.draw_value(V)\n",
        "\n",
        "# Plot policy for policy iteration\n",
        "#print(\"\\n\\nThe optimal policy using policy iteration is:\\n\\n {}\\n\\n\".format(policy))\n",
        "print(\"The graphical representation of the optimal policy using policy iteration is:\\n\")\n",
        "grid.graphics.draw_policy(policy)\n",
        "\n",
        "# Plot number of epochs\n",
        "print(\"\\nIt took {} epochs\".format(epochs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Suov-nzD5v6e"
      },
      "source": [
        "## Question 5:  impact of gamma on the policy iteration algorithm\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJP49aeCy8e9"
      },
      "source": [
        "### Question 5:  impact of gamma on the policy iteration algorithm\n",
        "\n",
        "gamma_range = [0, 0.2, 0.4, 0.6, 0.8]\n",
        "epochs = []\n",
        "policies = []\n",
        "values = []\n",
        "titles = []\n",
        "\n",
        "# Use policy iteration for each gamma value\n",
        "for gamma in gamma_range:\n",
        "    policy, V, epoch = grid.policy_iteration(threshold = 0.001, gamma = gamma)\n",
        "    epochs.append(epoch)\n",
        "    policies.append(policy)\n",
        "    values.append(V)\n",
        "    titles.append(\"gamma = {}\".format(gamma))\n",
        "\n",
        "# Plot the number of epochs vs gamma values\n",
        "print(\"Impact of gamma value on the number of epochs needed for the policy iteration algorithm:\\n\")\n",
        "plt.figure()\n",
        "plt.plot(gamma_range, epochs)\n",
        "plt.xlabel(\"Gamma range\")\n",
        "plt.ylabel(\"Number of epochs\")\n",
        "plt.show()\n",
        "\n",
        "# Print all value functions and policies for different values of gamma\n",
        "print(\"\\nGraphical representation of the value function for each gamma:\\n\")\n",
        "grid.graphics.draw_value_grid(values, titles, 1, 6)\n",
        "\n",
        "print(\"\\nGraphical representation of the policy for each gamma:\\n\")\n",
        "grid.graphics.draw_policy_grid(policies, titles, 1, 6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-hrUgit5zu9"
      },
      "source": [
        "## Question 6: value iteration implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9LOSONzq2_p"
      },
      "source": [
        "### Question 6: value iteration implementation\n",
        "policy, V, epochs = grid.value_iteration(threshold = 0.001, gamma = 0.7)\n",
        "\n",
        "# Plot value function for policy iteration\n",
        "#print(\"The value of the optimal policy computed using value iteration is:\\n\\n {}\\n\\n\".format(V))\n",
        "print(\"The graphical representation of the value of the optimal policy computed using value iteration is:\\n\")\n",
        "grid.graphics.draw_value(V)\n",
        "\n",
        "# Plot policy for value iteration\n",
        "#print(\"\\n\\nThe optimal policy computed using value iteration is:\\n\\n {}\\n\\n\".format(policy))\n",
        "print(\"The graphical representation of the optimal policy computed using value iteration is:\\n\")\n",
        "grid.graphics.draw_policy(policy)\n",
        "\n",
        "# Plot number of epoch\n",
        "print(\"\\nIt took {} epochs\".format(epochs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPkJeiMg52ev"
      },
      "source": [
        "## Question 7:  impact of gamma on the value iteration algorithm\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqU9MinAy7gq"
      },
      "source": [
        "### Question 7:  impact of gamma on the value iteration algorithm\n",
        "gamma_range = [0, 0.2, 0.4, 0.6, 0.8]\n",
        "epochs = []\n",
        "policies = []\n",
        "values = []\n",
        "titles = []\n",
        "\n",
        "# Use value iteration for each gamma value\n",
        "for gamma in gamma_range:\n",
        "    policy, V, epoch = grid.value_iteration(threshold = 0.001, gamma = gamma)\n",
        "    epochs.append(epoch)\n",
        "    policies.append(policy)\n",
        "    values.append(V)\n",
        "    titles.append(\"gamma = {}\".format(gamma))\n",
        "\n",
        "# Plot the number of epochs vs gamma values\n",
        "print(\"Impact of gamma value on the number of epochs needed for the value iteration algorithm:\\n\")\n",
        "plt.figure()\n",
        "plt.plot(gamma_range, epochs)\n",
        "plt.xlabel(\"Gamma range\")\n",
        "plt.ylabel(\"Number of epochs\")\n",
        "plt.show()\n",
        "\n",
        "# Print all value functions and policies for different values of gamma\n",
        "print(\"\\nGraphical representation of the value function for each gamma:\\n\")\n",
        "grid.graphics.draw_value_grid(values, titles, 1, 6)\n",
        "\n",
        "print(\"\\nGraphical representation of the policy for each gamma:\\n\")\n",
        "grid.graphics.draw_policy_grid(policies, titles, 1, 6)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}